# TTRC-231: Clustering Engine Implementation Plan

**Date:** 2025-10-13
**Status:** 📋 Ready to Start
**JIRA:** [TTRC-231](https://ajwolfe37.atlassian.net/browse/TTRC-231)
**Branch:** `test`
**Prerequisites:** TTRC-230 Complete ✅

---

## Context

**What TTRC-230 Delivered:**
- ✅ Hybrid scoring algorithm (6+ signals)
- ✅ Candidate generation (OR-blocking, <100ms)
- ✅ Centroid tracking (running average + nightly recompute)
- ✅ Core clustering logic (`clusterArticle()` in hybrid-clustering.js)
- ✅ Stale story reopening (score ≥0.80 validation)
- ✅ Job handlers (story.cluster, story.cluster.batch)

**What TTRC-231 Needs:**
1. **Lifecycle automation** - Auto-transition states (emerging → growing → stable → stale)
2. **Auto-split detection** - Identify when stories diverge (coherence <0.50)
3. **Periodic merge detection** - Daily job to merge duplicate stories (coherence >0.70)
4. **Clustering accuracy validation** - Test with 4 real Trump/Netanyahu articles
5. **Integration testing** - End-to-end test suite

**Key Insight:** The greedy clustering algorithm is ALREADY DONE. TTRC-231 is about lifecycle management + quality assurance.

---

## Implementation Tasks

### Task 1: Lifecycle Management Automation

**Objective:** Stories automatically transition between lifecycle states based on age

**Files to Create:**
- `scripts/rss/lifecycle.js` - Lifecycle update logic
- `migrations/025_lifecycle_automation.sql` - SQL function for state transitions

**Work Items:**
1. Create SQL function `update_story_lifecycle_states()`
   - Emerging: 0-6 hours since `last_updated_at`
   - Growing: 6-48 hours
   - Stable: 48-120 hours (5 days)
   - Stale: 5+ days
2. Create Node.js job handler for `story.lifecycle`
   - Calls SQL function
   - Logs state transitions
   - Returns count of updated stories
3. Add to job queue worker (`scripts/job-queue-worker.js`)
   - Map `story.lifecycle` to handler
4. Create GitHub Actions cron (`.github/workflows/lifecycle-update.yml`)
   - Runs hourly
   - Enqueues `story.lifecycle` job
5. Test manually with stories of various ages

**Acceptance Criteria:**
- [ ] SQL function updates all story states correctly
- [ ] Job handler runs without errors
- [ ] Cron triggers hourly
- [ ] State transitions logged

**Testing:**
- Create test stories with backdated timestamps
- Run lifecycle job
- Verify states transition correctly

---

### Task 2: Auto-Split Detection

**Objective:** Detect when a story contains unrelated articles and split into separate stories

**Files to Create:**
- `scripts/rss/auto-split.js` - Split detection logic
- Job handler in `scripts/job-queue-worker.js`

**Work Items:**
1. Implement `calculateInternalCoherence(story_id)`
   - Fetch all articles in story
   - Calculate pairwise cosine similarity (embeddings)
   - Sample if >20 articles (performance)
   - Return median coherence score
2. Implement `checkForSplit(story_id)`
   - Get coherence score
   - If <0.50, trigger split
   - Re-cluster articles using hybrid scoring
   - Create new stories as needed
3. Create job handler for `story.split`
   - On-demand trigger (manual or monitoring)
   - Returns split results
4. Add to job queue worker

**Acceptance Criteria:**
- [ ] Coherence calculation returns 0.0-1.0 score
- [ ] Split detection identifies diverging stories
- [ ] Articles reassigned to correct stories
- [ ] No data loss during split

**Testing:**
- Manually create story with 2 unrelated articles
- Run split detection
- Verify 2 stories created

**Edge Cases:**
- Story with <2 articles (skip split)
- All articles highly coherent (no split needed)
- Embeddings missing (handle gracefully)

---

### Task 3: Periodic Merge Detection

**Objective:** Daily job to identify and merge duplicate stories

**Files to Create:**
- `scripts/rss/periodic-merge.js` - Merge detection logic
- `migrations/026_merge_audit_trail.sql` - Audit table
- GitHub Actions cron (`.github/workflows/story-merge.yml`)

**Work Items:**
1. Create SQL migration for audit trail
   ```sql
   CREATE TABLE story_merge_actions (
     id BIGSERIAL PRIMARY KEY,
     source_story_id BIGINT NOT NULL,
     target_story_id BIGINT NOT NULL,
     coherence_score NUMERIC(5,3),
     shared_entities TEXT[],
     merged_at TIMESTAMPTZ DEFAULT NOW(),
     performed_by TEXT DEFAULT 'system'
   );
   ```
2. Implement `findMergeCandidates()`
   - Find story pairs with:
     - Coherence >0.70 (centroid similarity)
     - Share 3+ entities
     - Within 5-day time window
     - Same primary actor
   - Return ranked candidates
3. Implement `mergeStories(sourceId, targetId)`
   - Move article_story records
   - Update target story metadata
   - Recompute centroid
   - Mark source as merged (status = 'merged_into')
   - Insert audit record
4. Create job handler for `story.merge`
   - Find candidates
   - Merge top candidates (limit 10 per run)
   - Log results
5. Add to job queue worker
6. Create GitHub Actions cron
   - Runs daily at 2am
   - Enqueues `story.merge` job

**Acceptance Criteria:**
- [ ] Merge candidates identified correctly
- [ ] Stories merge without data loss
- [ ] Audit trail complete
- [ ] Daily job runs successfully

**Testing:**
- Manually create 2 duplicate stories
- Run merge detection
- Verify stories merged correctly
- Check audit trail

**Safety:**
- Never merge stories with different primary actors
- Require >3 shared entities (prevent false positives)
- Manual override available (admin tool for Phase 4)

---

### Task 4: Real Clustering Accuracy Test

**Objective:** Validate clustering with 4 real articles (Trump/Netanyahu pardon)

**Files to Modify:**
- `scripts/test-real-clustering.js` (EXISTS - needs fixing)

**Test Articles:**
1. https://www.politico.eu/article/more-than-cigars-and-champagne-donald-trump-benjamin-netanyahu-israel/
2. https://www.reuters.com/world/middle-east/trump-urges-israels-president-pardon-netanyahu-2025-10-13/
3. https://www.foxnews.com/world/trump-calls-netanyahu-pardon-after-hailing-swift-removal-left-wing-lawmakers-security
4. https://nypost.com/2025/10/13/us-news/trump-urges-israeli-president-to-pardon-netanyahu/

**Work Items:**
1. Fix test script to use proper RSS ingestion
   - Option A: Manually enqueue articles with embeddings
   - Option B: Add articles to test RSS feed
   - Option C: Create articles via API with full metadata extraction
2. Run clustering algorithm on all 4 articles
3. Validate results:
   - All 4 cluster to SAME story
   - Score breakdown logged
   - Performance <500ms p95
4. Calculate precision/recall metrics
5. Document in JIRA

**Acceptance Criteria:**
- [ ] 4 articles cluster to same story
- [ ] >85% clustering accuracy
- [ ] <500ms p95 latency
- [ ] Results documented in JIRA

**Expected Outcome:**
```
✅ SUCCESS: All articles clustered to the SAME story!
   Story ID: [story-id]

Similarity scores:
   Article 1: 1.000 (first article)
   Article 2: 0.847
   Article 3: 0.821
   Article 4: 0.859

Final story:
   ID: [story-id]
   Headline: "Trump urges Israeli president to pardon Netanyahu"
   Sources: 4
   State: emerging
```

---

### Task 5: Integration Testing

**Objective:** Comprehensive test suite for clustering engine

**Files to Create:**
- `tests/clustering-integration.test.js` - Integration tests
- `tests/lifecycle.test.js` - Lifecycle transition tests
- `tests/split-merge.test.js` - Split/merge tests

**Work Items:**
1. Create lifecycle transition tests
   - Create stories with various ages
   - Run lifecycle job
   - Assert correct state transitions
2. Create split detection tests
   - Story with unrelated articles
   - Run split detection
   - Assert stories split correctly
3. Create merge detection tests
   - Duplicate stories
   - Run merge job
   - Assert stories merged correctly
4. Create end-to-end clustering tests
   - Feed articles through pipeline
   - Verify story assignment
   - Check centroid updates
5. Add tests to package.json scripts
6. Add to CI/CD (GitHub Actions)

**Acceptance Criteria:**
- [ ] All tests pass locally
- [ ] Tests run in CI/CD
- [ ] No regressions in existing functionality
- [ ] Test coverage >80% for new code

**Test Structure:**
```javascript
describe('TTRC-231: Clustering Engine', () => {
  describe('Lifecycle Management', () => {
    test('transitions emerging → growing after 6h', async () => {});
    test('transitions growing → stable after 48h', async () => {});
    test('transitions stable → stale after 5d', async () => {});
  });

  describe('Auto-Split Detection', () => {
    test('splits story with coherence <0.50', async () => {});
    test('does not split coherent story', async () => {});
  });

  describe('Periodic Merge', () => {
    test('merges duplicate stories', async () => {});
    test('does not merge unrelated stories', async () => {});
  });

  describe('End-to-End Clustering', () => {
    test('clusters 4 related articles correctly', async () => {});
    test('creates new story for unrelated article', async () => {});
  });
});
```

---

## Database Migrations

### Migration 025: Lifecycle Automation
```sql
-- Add lifecycle state enum if not exists
-- (Should already exist from TTRC-230, but verify)

-- Create function to update all story lifecycle states
CREATE OR REPLACE FUNCTION update_story_lifecycle_states()
RETURNS TABLE (
  updated_count INTEGER,
  state_counts JSONB
)
LANGUAGE plpgsql
AS $$
DECLARE
  v_updated_count INTEGER;
  v_state_counts JSONB;
BEGIN
  -- Update all story lifecycle states based on last_updated_at
  UPDATE stories
  SET lifecycle_state = CASE
    WHEN last_updated_at > NOW() - INTERVAL '6 hours' THEN 'emerging'
    WHEN last_updated_at > NOW() - INTERVAL '48 hours' THEN 'growing'
    WHEN last_updated_at > NOW() - INTERVAL '120 hours' THEN 'stable'
    ELSE 'stale'
  END
  WHERE lifecycle_state != CASE
    WHEN last_updated_at > NOW() - INTERVAL '6 hours' THEN 'emerging'
    WHEN last_updated_at > NOW() - INTERVAL '48 hours' THEN 'growing'
    WHEN last_updated_at > NOW() - INTERVAL '120 hours' THEN 'stable'
    ELSE 'stale'
  END;

  GET DIAGNOSTICS v_updated_count = ROW_COUNT;

  -- Count stories by state
  SELECT jsonb_object_agg(lifecycle_state, count)
  INTO v_state_counts
  FROM (
    SELECT lifecycle_state, COUNT(*)::INTEGER as count
    FROM stories
    GROUP BY lifecycle_state
  ) counts;

  RETURN QUERY SELECT v_updated_count, v_state_counts;
END;
$$;

COMMENT ON FUNCTION update_story_lifecycle_states() IS
'TTRC-231: Update all story lifecycle states based on last_updated_at timestamp';
```

### Migration 026: Merge Audit Trail
```sql
-- Create audit table for story merge actions
CREATE TABLE IF NOT EXISTS story_merge_actions (
  id BIGSERIAL PRIMARY KEY,
  source_story_id BIGINT NOT NULL REFERENCES stories(id),
  target_story_id BIGINT NOT NULL REFERENCES stories(id),
  coherence_score NUMERIC(5,3),
  shared_entities TEXT[],
  merged_at TIMESTAMPTZ DEFAULT NOW(),
  performed_by TEXT DEFAULT 'system',
  reason TEXT
);

CREATE INDEX idx_story_merge_actions_source ON story_merge_actions(source_story_id);
CREATE INDEX idx_story_merge_actions_target ON story_merge_actions(target_story_id);
CREATE INDEX idx_story_merge_actions_merged_at ON story_merge_actions(merged_at DESC);

COMMENT ON TABLE story_merge_actions IS
'TTRC-231: Audit trail for story merge operations';

-- Add 'merged_into' status for stories
-- (May need to modify stories.status enum)
ALTER TYPE story_status ADD VALUE IF NOT EXISTS 'merged_into';
```

---

## GitHub Actions Workflows

### Lifecycle Update Cron
**File:** `.github/workflows/lifecycle-update.yml`

```yaml
name: Story Lifecycle Update

on:
  schedule:
    - cron: '0 * * * *'  # Every hour
  workflow_dispatch:  # Manual trigger

jobs:
  update-lifecycle:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm ci

      - name: Enqueue lifecycle update job
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: node scripts/enqueue-lifecycle-job.js
```

### Story Merge Cron
**File:** `.github/workflows/story-merge.yml`

```yaml
name: Story Merge Detection

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2am UTC
  workflow_dispatch:  # Manual trigger

jobs:
  merge-stories:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm ci

      - name: Run merge detection
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: node scripts/enqueue-merge-job.js
```

---

## Success Metrics

### Clustering Quality (from JIRA)
- **Precision:** ≥0.90 (articles in same story truly related)
- **Recall:** ≥0.85 (related articles cluster together)
- **Manual intervention:** <5% of stories need manual action

### Performance (from JIRA)
- **Clustering latency:** p95 <500ms per article
- **Lifecycle update:** <10s for all stories (hourly)
- **Merge detection:** <60s daily job

### Cost
- **$0** - No new OpenAI calls, uses existing infrastructure

---

## Process Workflow (MUST FOLLOW)

### Session Start
1. ✅ Read this handoff document
2. ✅ Read `/docs/STARTUP_PROMPT.md`
3. ✅ Check JIRA TTRC-231 status
4. ✅ Confirm on TEST branch
5. ✅ Create todo list (use TodoWrite tool)

### Development Loop
1. Pick task from todo list
2. Mark as in_progress
3. Write code
4. Test locally (run test scripts)
5. Mark as completed
6. Move to next task

### Before Committing
1. Run validation agent (Task tool with general-purpose)
2. Run all tests manually
3. Verify no regressions
4. Check code quality

### Commit & Push
1. Stage files: `git add [files]`
2. Commit with message following convention
3. Push to TEST branch: `git push origin test`
4. Wait for AI code review (GitHub Actions)

### After AI Review
1. Check workflow results
2. Fix any blockers
3. Re-push if needed
4. Verify all checks pass

### Session End
1. Update JIRA with progress
2. Update this handoff with status
3. Create new handoff if work incomplete
4. Mark JIRA as "Ready for Prod" if complete

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Lifecycle transitions lag | Low | Medium | Hourly cron, sub-10s execution, monitoring |
| Over-splitting stories | Medium | Medium | Conservative threshold (0.50), manual review |
| Over-merging stories | Medium | High | Strict criteria (0.70, 3+ entities, same actor) |
| Performance degradation | Low | Medium | Batch operations, sampling for large stories |
| Test articles don't cluster | Low | High | Use proper RSS ingestion with embeddings |

---

## Dependencies

### Infrastructure (All Complete ✅)
- ✅ Supabase TEST database (PostgreSQL 15+)
- ✅ pgvector extension installed
- ✅ HNSW indexes created
- ✅ OpenAI API configured

### Code (All Complete ✅)
- ✅ Hybrid scoring (`scripts/rss/scoring.js`)
- ✅ Candidate generation (`scripts/rss/candidate-generation.js`)
- ✅ Centroid tracking (`scripts/rss/centroid-tracking.js`)
- ✅ Core clustering (`scripts/rss/hybrid-clustering.js`)
- ✅ Job handlers (`scripts/story-cluster-handler.js`)

### Testing (To Be Created)
- ⏳ Integration test suite
- ⏳ Lifecycle tests
- ⏳ Split/merge tests
- ⏳ Real article clustering test

---

## Files to Create/Modify

### New Files
- `scripts/rss/lifecycle.js` - Lifecycle management
- `scripts/rss/auto-split.js` - Split detection
- `scripts/rss/periodic-merge.js` - Merge detection
- `migrations/025_lifecycle_automation.sql` - SQL function
- `migrations/026_merge_audit_trail.sql` - Audit table
- `tests/clustering-integration.test.js` - Integration tests
- `tests/lifecycle.test.js` - Lifecycle tests
- `tests/split-merge.test.js` - Split/merge tests
- `scripts/enqueue-lifecycle-job.js` - Cron helper
- `scripts/enqueue-merge-job.js` - Cron helper
- `.github/workflows/lifecycle-update.yml` - Hourly cron
- `.github/workflows/story-merge.yml` - Daily cron

### Files to Modify
- `scripts/job-queue-worker.js` - Add lifecycle/split/merge handlers
- `scripts/test-real-clustering.js` - Fix to use proper ingestion
- `package.json` - Add test scripts

### Files to Read (Context)
- `scripts/rss/hybrid-clustering.js` - Understand clustering flow
- `scripts/rss/centroid-tracking.js` - Understand centroid updates
- `scripts/rss/scoring.js` - Understand scoring formula
- `scripts/story-cluster-handler.js` - Understand job pattern

---

## Testing Strategy

### Unit Tests
- Coherence calculation (pairwise similarity)
- Merge candidate detection (filtering logic)
- Lifecycle state transitions (time-based logic)

### Integration Tests
- Full clustering pipeline (article → story)
- Lifecycle update job (SQL + handler)
- Split detection (multi-article stories)
- Merge detection (duplicate stories)

### Manual Tests
- Test with 4 Trump/Netanyahu articles
- Create stories with various ages
- Create diverging story (unrelated articles)
- Create duplicate stories

### QA Checklist
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] Manual test with 4 articles passes
- [ ] Performance <500ms p95
- [ ] No regressions in existing features
- [ ] Lifecycle cron works
- [ ] Merge cron works
- [ ] AI code review passes

---

## Estimated Time

**Total:** 3-4 days (per JIRA spec)

**Breakdown:**
- Lifecycle management: 1 day
  - SQL migration: 2 hours
  - Job handler: 2 hours
  - Cron setup: 1 hour
  - Testing: 3 hours
- Auto-split detection: 1 day
  - Coherence calculation: 3 hours
  - Split logic: 3 hours
  - Testing: 2 hours
- Periodic merge: 1 day
  - Candidate detection: 3 hours
  - Merge execution: 3 hours
  - Audit trail: 1 hour
  - Cron setup: 1 hour
- Testing & validation: 1 day
  - Real article test: 2 hours
  - Integration tests: 4 hours
  - QA & fixes: 2 hours

---

## Next Phase After TTRC-231

**TTRC-232: Admin & Monitoring**
- Manual merge/split tools
- Duplicate detection (SimHash)
- Quality dashboard (B-cubed F1)
- Admin documentation

---

## Questions to Resolve

1. **Lifecycle cron frequency:** Hourly OK or too frequent?
   - **Answer:** Hourly is fine, stories don't change state that often
2. **Merge threshold:** 0.70 coherence strict enough?
   - **Answer:** Start strict, relax if too many false negatives
3. **Split threshold:** 0.50 coherence too aggressive?
   - **Answer:** Conservative, most stories should be >0.60
4. **Test articles:** Use real RSS or manual insertion?
   - **Answer:** Prefer real RSS ingestion for proper embeddings

---

**Document Status:** Ready for Implementation
**Last Updated:** 2025-10-13
**Created By:** Claude Code (Session ending, handoff to new session)
**Next Session:** Start with this document + todo list
